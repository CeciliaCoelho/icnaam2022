{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Coelho, M. F. P. Costa, and L.L. Ferrás, \"The role of adaptive activation functions in Fractional Physics-Informed Neural Networks\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7gbazrFIcvG2"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/CeciliaCoelho/fPINNs.git\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install scikit-optimize matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we aim to analyse the effect of adaptive activation functions on the convergence rate and solution\n",
    "accuracy of fPINNs. For this purpose, we consider the numerical solution of the fractional Laplacian equation (FLE),\n",
    "\n",
    "\\begin{equation}\n",
    "    (-\\Delta)^{\\alpha/2}u(x)=f(x),~~~u(0)=u(1)=0,~~~x\\in(0,1),~\\alpha\\in(1,2) \\tag{1}\n",
    "\\end{equation}\n",
    "where $(-\\Delta)^{\\alpha/2}$ is the fractional Laplacian operator (see <a href=\"http://example.com/\" target=\"_blank\">link</a> for more details). <span style=\"color:red\">add link to go to icnaam 2021 paper</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we analyse the performance of fPINNs when one adaptive parameter per layer is used in solving the\n",
    "1D Fractional Laplacian Equation with a non-smooth analytical solution. Moreover, we analyse the limitations of\n",
    "fPINNs when solving the FLE with a series of gradually smaller non-integer order α values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dt03s9DJ2el0",
    "outputId": "8cef8f62-0240-4aab-bf2b-85229b306c2c"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import deepxde as dde\n",
    "from deepxde.backend import tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLbF4giW2el-"
   },
   "source": [
    "We consider the numerical solution of (1) with $f(x)=\\left(2\\cos\\left(\\frac{\\pi\\alpha}{2}\\right)\\right)\\Gamma (\\alpha + 2)x$. The analytical solution is not smooth, and is given by $u(x)=x(1-x^2)^{\\alpha /2}$. As we have seen in an earlier work \\cite{ICNAAM2021}, this case is more extreme and should present greater difficulties in approximating the fractional operator (through the difference scheme) and in solving the equation by fPINN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yTUJOw_B2el_"
   },
   "outputs": [],
   "source": [
    "def NN(activation, optimizer, initializer, alpha):\n",
    "    \n",
    "    def fle(x, y, int_mat):\n",
    "        # defining the fractional Laplacian equation(1)\n",
    "        if isinstance(int_mat, (list, tuple)) and len(int_mat) == 3:\n",
    "            int_mat = tf.SparseTensor(*int_mat)\n",
    "            lhs = tf.sparse_tensor_dense_matmul(int_mat, y)\n",
    "        else:\n",
    "          \n",
    "          lhs = tf.matmul(int_mat, y)\n",
    "         \n",
    "        lhs /= 2 * np.cos(alpha * np.pi / 2)\n",
    "        rhs = gamma(alpha + 2) * x\n",
    "        return lhs - rhs[: tf.size(lhs)]\n",
    "\n",
    "    def NNSol(x):\n",
    "        # defining the forced non-smooth numerical solution\n",
    "        return x * (np.abs(1 - x**2)) ** (alpha / 2)\n",
    "        \n",
    "\n",
    "    geom = dde.geometry.Interval(0, 1)\n",
    "    bc = dde.DirichletBC(geom, NNSol, lambda _, on_boundary: on_boundary)\n",
    "\n",
    "\n",
    "    data = dde.data.FPDE(geom, fle, alpha, bc, [101], meshtype=\"static\", solution=NNSol, num_domain=40)\n",
    "    \n",
    "    # network architecture\n",
    "    net = dde.maps.FNN([1] + [128] + [64] + [32] + [16] + [8] + [1], activation, initializer)\n",
    "    net.apply_output_transform(lambda x, y: x * (1 - x) * y)\n",
    "\n",
    "    # training\n",
    "    model = dde.Model(data, net)\n",
    "    model.compile(optimizer, metrics=[\"l2 relative error\"])\n",
    "    losshistory, train_state = model.train(epochs=50000)\n",
    "\n",
    "    # testing\n",
    "    X = geom.random_points(1000)\n",
    "    y_true = NNSol(X)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    model.print_model()\n",
    "    \n",
    "    return losshistory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered a feed-forward NN with 5 hidden layers and a configuration of 128-64-32-16-8 neurons.\n",
    "In our work training waas done with different combinations of hyperparameters:\n",
    "- activation function: tanh, silu;\n",
    "- optimizer: L-BFGS;\n",
    "- initializer (weights): Glorot Normal with mean and standard deviation given respectively by $0$ and  $\\sqrt{\\frac{2}{n^{k}+n^{k-1}}}$ ($n^j$ is the number of neurons in the layer $j=k-1,k$) and uniform distributions given by $U(-1,1)$ and $U(-2,2)$;\n",
    "- alpha: 1.9,1.8,1.7,1.6,1.5,1.4,1.3,1.2;\n",
    "\n",
    "we implemented a multi-start strategy to avoid being trapped in a local optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiStart(n, activation, optimizer, initializer, l_alpha):\n",
    "    res = {}\n",
    "    for j in range(len(l_alpha)):\n",
    "        res[str(l_alpha[j])] = []\n",
    "        for i in range(n):\n",
    "            res[str(l_alpha[j])].append(NN(activation, optimizer, initializer, l_alpha[j]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_alpha = [1.9]#,1.8,1.7,1.6,1.5,1.4,1.3,1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data\n",
    "\n",
    "we can organize the L2 relative errors by alpha value in a dataframe by using simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize(results):\n",
    "    for k, v in results.items():\n",
    "        results[k] = list(map(lambda x: x.metrics_test[1][0], v))\n",
    "    print(pd.DataFrame.from_dict(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VyH7vAW52el_",
    "outputId": "3cfd2a1c-56d2-4835-b6a2-51600ab8acc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.069387 s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/fPINNs/deepxde/nn/tensorflow_compat_v1/fnn.py:103: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'compile' took 0.519487 s\n",
      "\n",
      "Initializing variables...\n",
      "!!!!Training model...\n",
      "\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "Step      Train loss              Test loss               Test metric   \n",
      "0         [8.17e+00, 0.00e+00]    [8.17e+00, 0.00e+00]    [9.44e-01]    \n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.131728\n",
      "  Number of iterations: 30\n",
      "  Number of functions evaluations: 54\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "54        [1.32e-01, 0.00e+00]    [1.32e-01, 0.00e+00]    [7.56e-03]    \n",
      "\n",
      "Best model at step 54:\n",
      "  train loss: 1.32e-01\n",
      "  test loss: 1.32e-01\n",
      "  test metric: [7.56e-03]\n",
      "\n",
      "'train' took 0.763950 s\n",
      "\n",
      "final adaptive parameters values: [0.5958619, 0.58940727, 0.60133773, 1.0965881, 2.0483937]\n",
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.051355 s\n",
      "\n",
      "'compile' took 0.204468 s\n",
      "\n",
      "Initializing variables...\n",
      "!!!!Training model...\n",
      "\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "Step      Train loss              Test loss               Test metric   \n",
      "0         [8.19e+00, 0.00e+00]    [8.19e+00, 0.00e+00]    [9.43e-01]    \n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.501545\n",
      "  Number of iterations: 20\n",
      "  Number of functions evaluations: 39\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "39        [5.02e-01, 0.00e+00]    [5.02e-01, 0.00e+00]    [5.68e-02]    \n",
      "\n",
      "Best model at step 39:\n",
      "  train loss: 5.02e-01\n",
      "  test loss: 5.02e-01\n",
      "  test metric: [5.68e-02]\n",
      "\n",
      "'train' took 0.321465 s\n",
      "\n",
      "final adaptive parameters values: [0.66581804, 0.5829369, 0.6601408, 0.9450137, 1.3124213]\n",
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.044382 s\n",
      "\n",
      "'compile' took 0.246223 s\n",
      "\n",
      "Initializing variables...\n",
      "!!!!Training model...\n",
      "\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "Step      Train loss              Test loss               Test metric   \n",
      "0         [1.18e+01, 0.00e+00]    [1.18e+01, 0.00e+00]    [1.11e+00]    \n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.144365\n",
      "  Number of iterations: 32\n",
      "  Number of functions evaluations: 59\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "59        [1.44e-01, 0.00e+00]    [1.44e-01, 0.00e+00]    [1.35e-02]    \n",
      "\n",
      "Best model at step 59:\n",
      "  train loss: 1.44e-01\n",
      "  test loss: 1.44e-01\n",
      "  test metric: [1.35e-02]\n",
      "\n",
      "'train' took 0.681804 s\n",
      "\n",
      "final adaptive parameters values: [1.1141286, 1.1906726, 1.1554681, 1.0337492, 1.3776566]\n",
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.045439 s\n",
      "\n",
      "'compile' took 0.273593 s\n",
      "\n",
      "Initializing variables...\n",
      "!!!!Training model...\n",
      "\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "Step      Train loss              Test loss               Test metric   \n",
      "0         [8.99e+00, 0.00e+00]    [8.99e+00, 0.00e+00]    [9.85e-01]    \n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.120669\n",
      "  Number of iterations: 34\n",
      "  Number of functions evaluations: 74\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "74        [1.21e-01, 0.00e+00]    [1.21e-01, 0.00e+00]    [1.22e-02]    \n",
      "\n",
      "Best model at step 74:\n",
      "  train loss: 1.21e-01\n",
      "  test loss: 1.21e-01\n",
      "  test metric: [1.22e-02]\n",
      "\n",
      "'train' took 0.756609 s\n",
      "\n",
      "final adaptive parameters values: [0.9837868, 1.0446457, 0.9697188, 1.1540319, 1.5349354]\n",
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "'build' took 0.047433 s\n",
      "\n",
      "'compile' took 0.321282 s\n",
      "\n",
      "Initializing variables...\n",
      "!!!!Training model...\n",
      "\n",
      "<built-in function vars>\n",
      "<built-in function vars>\n",
      "Step      Train loss              Test loss               Test metric   \n",
      "0         [7.25e+00, 0.00e+00]    [7.25e+00, 0.00e+00]    [8.94e-01]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rGlorotTanh \u001b[38;5;241m=\u001b[39m \u001b[43mmultiStart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLAAF-1 tanh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGlorot normal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_alpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mmultiStart\u001b[0;34m(n, activation, optimizer, initializer, l_alpha)\u001b[0m\n\u001b[1;32m      4\u001b[0m     res[\u001b[38;5;28mstr\u001b[39m(l_alpha[j])] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m----> 6\u001b[0m         res[\u001b[38;5;28mstr\u001b[39m(l_alpha[j])]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_alpha\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mNN\u001b[0;34m(activation, optimizer, initializer, alpha)\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m dde\u001b[38;5;241m.\u001b[39mModel(data, net)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2 relative error\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m     37\u001b[0m X \u001b[38;5;241m=\u001b[39m geom\u001b[38;5;241m.\u001b[39mrandom_points(\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/utils/internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, te \u001b[38;5;241m-\u001b[39m ts))\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/model.py:440\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, epochs, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizers\u001b[38;5;241m.\u001b[39mis_external_optimizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_name):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow.compat.v1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_tensorflow_compat_v1_scipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_tensorflow_tfp()\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/model.py:503\u001b[0m, in \u001b[0;36mModel._train_tensorflow_compat_v1_scipy\u001b[0;34m(self, display_every)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_next_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size))\n\u001b[1;32m    497\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mfeed_dict(\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mX_train,\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_train,\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mtrain_aux_vars,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test()\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/optimizers/tensorflow_compat_v1/scipy_optimizer.py:181\u001b[0m, in \u001b[0;36mExternalOptimizerInterface.minimize\u001b[0;34m(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m initial_packed_var_val \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_var)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Perform minimization.\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m packed_var_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_packed_var_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_grad_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_grad_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mequality_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequality_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mequality_grad_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequality_grad_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minequality_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minequality_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43minequality_grad_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minequality_grad_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m var_vals \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    193\u001b[0m     packed_var_val[packing_slice] \u001b[38;5;28;01mfor\u001b[39;00m packing_slice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packing_slices\n\u001b[1;32m    194\u001b[0m ]\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Set optimization variables to their new values.\u001b[39;00m\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/optimizers/tensorflow_compat_v1/scipy_optimizer.py:382\u001b[0m, in \u001b[0;36mScipyOptimizerInterface._minimize\u001b[0;34m(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m minimize_kwargs\u001b[38;5;241m.\u001b[39mupdate(optimizer_kwargs)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mminimize_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mminimize_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m message_lines \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization terminated with:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Message: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Objective function value: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    388\u001b[0m ]\n\u001b[1;32m    389\u001b[0m message_args \u001b[38;5;241m=\u001b[39m [result\u001b[38;5;241m.\u001b[39mmessage, result\u001b[38;5;241m.\u001b[39mfun]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    696\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    697\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    702\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    356\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124;03m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/optimizers/tensorflow_compat_v1/scipy_optimizer.py:344\u001b[0m, in \u001b[0;36mScipyOptimizerInterface._minimize.<locals>.loss_grad_func_wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_grad_func_wrapper\u001b[39m(x):\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m# SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m     loss, gradient \u001b[38;5;241m=\u001b[39m \u001b[43mloss_grad_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, gradient\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tf/fPINNs/deepxde/optimizers/tensorflow_compat_v1/scipy_optimizer.py:268\u001b[0m, in \u001b[0;36mExternalOptimizerInterface._make_eval_func.<locals>.eval_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    265\u001b[0m augmented_feed_dict\u001b[38;5;241m.\u001b[39mupdate(feed_dict)\n\u001b[1;32m    266\u001b[0m augmented_fetches \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;241m+\u001b[39m fetches\n\u001b[0;32m--> 268\u001b[0m augmented_fetch_vals \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_fetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmented_feed_dict\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(callback):\n\u001b[1;32m    273\u001b[0m     callback(\u001b[38;5;241m*\u001b[39maugmented_fetch_vals[num_tensors:])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    964\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    970\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:1190\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1190\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1193\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:1370\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1370\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:1377\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1376\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1379\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:1360\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1358\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py:1453\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1452\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1453\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rGlorotTanh = multiStart(5, 'LAAF-1 tanh', 'L-BFGS',\"Glorot normal\", l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rGlorotTanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l1xTH1p2emA",
    "outputId": "3ae29528-b849-4580-ecf5-dc973a6db744"
   },
   "outputs": [],
   "source": [
    "rGlorotSilu = multiStart(5, 'LAAF-1 silu', 'L-BFGS',\"Glorot normal\",l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rGlorotSilu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random uniform [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyH7vAW52el_",
    "outputId": "3cfd2a1c-56d2-4835-b6a2-51600ab8acc6"
   },
   "outputs": [],
   "source": [
    "rRandom1Tanh = multiStart(5, 'LAAF-1 tanh', 'L-BFGS',initializer,l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rRandom1Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l1xTH1p2emA",
    "outputId": "3ae29528-b849-4580-ecf5-dc973a6db744"
   },
   "outputs": [],
   "source": [
    "rRandom1Silu = multiStart(5, 'LAAF-1 silu', 'L-BFGS',initializer,l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rRandom1Silu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random uniform [-2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=-2, maxval=2, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyH7vAW52el_",
    "outputId": "3cfd2a1c-56d2-4835-b6a2-51600ab8acc6"
   },
   "outputs": [],
   "source": [
    "rRandom2Tanh = multiStart(5, 'LAAF-1 tanh', 'L-BFGS',initializer,l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rRandom2Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l1xTH1p2emA",
    "outputId": "3ae29528-b849-4580-ecf5-dc973a6db744"
   },
   "outputs": [],
   "source": [
    "rRandom2Silu = multiStart(5, 'LAAF-1 silu', 'L-BFGS',initializer,l_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize(rRandom2Silu)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fPINNsAdaptive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
